\documentclass[11pt]{article}
%Gummi|065|=)
\title{Homework \#2: Discussion on kNN and Decision Trees\\ for Classifying CNN News Articles}
\author{Doug McGeehan\\
		CS 6001: Applied Spatial and \\ Temporal Data Analysis\\
		Spring 2017}

\usepackage[margin=1in]{geometry}
\usepackage{verbatim}
\usepackage{framed}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\begin{document}

\maketitle

\section{Experimental Setup}

100 articles from the CNN website were transformed into term frequency and tf-idf matrices for this report's analyses.
These articles were selected randomly in such a way as to have an equal portion of articles in each category: since there are seven categories present in the CNN article dataset, each category in the subset of 100 articles consists of 14 to 15 articles.
The results from each experiment were averaged through 5-fold cross validation,
 whereby the dataset was partitioned into five equal-sized subsets, four used for classifier training and one used for classifier testing.
This was repeated five times for all combinations of testing and training partition assignment.
The \texttt{scikit-learn} Python library was used to calculate the precision, recall, and f-scores of each classification task.

\section{Performance of Decision Trees} \label{sec:tree}

The \texttt{DecisionTreeClassifier} of the \texttt{scikit-learn} Python library was used to perform the experiments on decision tree supervised classification.
This package creates binary-splitting decision trees using either the Gini index or information entropy gain as the criteria for performing node splits.
Once a decision tree is trained, the classes of the articles in the testing dataset are predicted by traversing the tree, with the predicted class compared against its actual class.
Additionally, the path over which the article traversed through the tree is recorded so that the classification difficulty of each class is quantified.


\begin{figure}[h!] \label{fig:pathlengths}
  \centering
  \includegraphics[width=\textwidth]{figures/decision_tree/path_depths}
  \caption{For each article category, the aggregate path lengths over which articles traverse is illustrated.
  Each bar corresponds to the average path lengths in a tree, differentiated by how it was constructed: node splitting criterion (entropy or Gini index) and dataset type (term frequency or tf-idf matrices).
  Error lines correspond to the minimum and maximum path lengths.}
\end{figure}


Figure 1 illustrates the average, minimum, and maximum length of the paths through which an article of a given category traversed to be classified.
From this figure, it is observed that articles from the entertainment, travel, and technology categories required longer paths to perform classifications both on average and at maximum length.
This implies these categories are harder to classify than others.
Beyond those categories, articles from politics, living, and health were the most difficult to classify under entropy-split trees built from the tf-idf representation of the articles.
Crime articles, on the other hand, were the easiest to classify from these trees.
For trees built on term frequency datasets, health and political articles were the easiest to classify.

Figures 2a and 2b illustrate the performance metrics obtained using term frequency and tf-idf representations for training, respectively.
Accuracy, precisions, recalls, and f-scores varied from one criterion-splitting tree to another without a significant overall pattern.

\begin{figure}[h!] \label{fig:perftf_decisiontree}
	\centering
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=\linewidth]{figures/decision_tree/tf_prec_n_rec}
	  \caption{Performance metrics for Decision Trees trained \\
	  on Term Frequency matrices.}
	  \label{fig:sub1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=\linewidth]{figures/decision_tree/tfidf_prec_n_rec}
	  \caption{Performance metrics for Decision Trees trained on Tf-idf matrices.}
	  \label{fig:sub2}
	\end{subfigure}
	\caption{Category-specific performance metrics obtained by Decision Tree classification.}
\end{figure}

For decision trees trained on term frequency datasets, accuracies in classification were moderate to low.
Health and crime articles received relatively more accurate classifications than other categories by achieving accuracies above 50\%, with crime articles resulting in the highest accuracy of $\approx$60\% on entropy-split trees.
Travel and living articles, however, were correctly classified at the poorest rate, with performance not exceeding 20\% on any metric (with the exception of travel articles on an entropy-split tree gaining $\approx$30\%).
It should be noted that articles from the living category were successsfully classified by an entropy-splitting tree with approximately the same results as random classification.
Its accuracy, precision, and recall did not exceed 17\%.

Decision trees trained on tf-idf representations of the dataset performed slightly lower than those trained on term frequency representations.
Across all article categories, accuracy and recall did not exceed 50\%.
Politics achieved the highest precision at 58\% under Gini-splitting trees, whereas health articles consistenty obtained an accuracy and recall of $\approx$50\% and a precision of $\approx$57\% regardless of the splitting criteria.
With the exception of health articles under an entropy-splitting tree, recalls did not exceed 40\%.
Finally, travel and living articles were classified with the worst performance overall, with those on a Gini-splitting tree performing worse than random guessing.


\section{Discussion on k-NN Classification} \label{sec:knn}

The \texttt{KNeighborsClassifier} of the \texttt{scikit-learn} Python library was used to perform the experiments on $k$-Nearest Neighbors ($k$-NN) supervised classification.
This package measures the distance of an article to be classified against the set of articles with known classes, and predicts the article's class based on its $k$-nearest neighbors.
The distance metric and neighbor voting weights are configurable.
For this analyses, voting was varied between uniformly weighted and distance weighted among all $k$ neighbors.
For calculating the distance between two neighbors, the Jaccard distance (using the \texttt{jaccard} function defined in the \texttt{scipy.spatial.distance} module), cosine distance (\texttt{cosine} from \texttt{scipy.spatial.distance}), and Euclidean distance (defined in \texttt{scikit-learn} were chosen for \texttt{KNeighborsClassifier}).
The vectors to be trained and tested on were either term frequency vectors representing the articles or tf-idf vectors.

\begin{figure}[h!] \label{fig:perf_knn}
	\centering
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=\linewidth]{figures/knn/accuracies}
	  \caption{Accuracies obtained by varying the $k$ for $k$-NN\\
	   uniform classification between different vector \\ types and different distance metrics.}
	  \label{fig:knn_accuracies}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=\linewidth]{figures/knn/fmeasures}
	  \caption{F-measures obtained by varying the $k$ for $k$-NN\\
	   uniform classification between different vector types and different distance metrics, differentiated by the categories to be classified.}
	  \label{fig:knn_fmeasures}
	\end{subfigure}
	\caption{Accuracies and F-measures obtained from the $k$-NN uniform voting weight series of experiments.}
\end{figure}

Figures 3a and 3b illustrate the performance changes under varying values of $k$ when uniform voting is chosen for neighbor classification.
Figures 4a and 4b illustrate the same, but with distance-weighted voting.
The goal of this experiment was to identify the \emph{optimal} $k$ value for performing supervised classifications (Figure 3a, 4a), as well as observe the f-measures for each particular article category (Figure 3b, 4b).

\begin{figure}[h!] \label{fig:perf_knn_distance}
	\centering
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=\linewidth]{figures/knn/distance_voting/accuracies}
	  \caption{Accuracies obtained by varying the $k$ for $k$-NN\\
	   distance-weighted classification between different \\
	   vector types and different distance metrics.}
	  \label{fig:knn_accuracies}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=\linewidth]{figures/knn/distance_voting/fmeasures}
	  \caption{F-measures obtained by varying the $k$ for $k$-NN\\
	   distance-weighted classification between different vector types and different distance metrics, differentiated by the categories to be classified.}
	  \label{fig:knn_fmeasures}
	\end{subfigure}
	\caption{Accuracies and F-measures obtained from the $k$-NN distance-weighted voting series of experiments.}
\end{figure}

In the previous report, it was found that the cosine similarity between term frequency vectors performed the best over the Jaccard similarity and Euclidean distance, with Euclidean performing the poorest.
This is corroborated in the bottom plot of Figure 3a and 4a.
However, the top plots indicate that transforming a term frequency vector into a tf-idf vector brings Euclidean distance almost equal to that of cosine distance.
The Jaccard distance performs the poorest when used between tf-idf vectors, though this is attributed to it not being suitable for real-valued vectors.

\subsection{Optimal $k$}

By varying the value of $k$, the best performance is achieved at $k=5$ and $8$ for tf-idf vectors, with performance dipping slightly at $8 \le k \le 10$ when considering uniform voting weights.
For term frequency vectors, the best performance seems to come at $k=4$ for any configuration.
Intuitively, this suggests that $k<4$ is susceptible to noise and poor correlations between feature occurrences in nearby articles.
For $k>4$, the drop of performance of term frequency vectors that some articles are being incorrectly classified by further-away neighbors of different classes.
This behavior is more pronounced when uniform weights were used for classifying a given article, as the accuracies illustrated in Figure 4a appear to remain consistent or growing for $k>4$.

\subsection{Impact of $k$ on F-measures}

Looking at Figure 3b and 4b, the f-measures for each classification is plotted against the changes to $k$.
Overall, travel, entertainment, and living articles receive the worst classifications in both their term frequency and tf-idf representations, implying they have lower precision and recalls.
Politic, crime, and technology articles show consistently high f-measures through each configuration of the experiments.

For $k\ge7$, the precisions and recalls of most categories begin to drop, which impacts the accuracies illustrated in Figures 3a and 4a.
Articles from entertainment, health, living, and travel begin to suffer noticeably.
Classification on crime articles, on the other hand, appears to steadily improve with increasing values of $k$ under distance-weighted voting.
Politics, crime, health, and tech appear to have the highest f-measures at higher values of $k$.

For low $k$ values, the tf-idf representation of technology and politics articles achieves better classifications over others.
Crime starts off with poor performance under distance-weighted voting, but achieves good initial performance under uniform-weighted voting.


\begin{figure}[h!] \label{fig:heatmap_neighborhoods}
  \centering
  \includegraphics[width=\textwidth]{figures/knn/knn_heatmaps}
  \caption{}
\end{figure}

Figure 5 presents an illustration of the feature occurrences between two articles and the articles within their nearest neighbors.
This figure was created from the experment on 5-nearest neighbors using Euclidean distance between tf-idf vectors, of which Figure 3 indicates this configuration achieved the very good performance.
For each figure, the first article represents the article to be classified, with the remaining five articles being within each article's $k$-neighborhood.

The top figure shows the article with the most densely populated neighborhood - i.e. the article with $k$ neighbors whose summed distances is minimum across all other articles.
From this figure, we see that it's closest neighbor is within the same class, and shares many common features, the most prominent being the term \emph{drug}.
It is interesting to observe that the remaining neighbors are relatively far away, indicative of their fewer common features and their different categories.
This implies that the tf-idf vector space is relatively sparse.
However, each article still shares some prominent features, such as \emph{women} and \emph{sexual}, which results in them being closer to the considered article than all others.

The bottom figure shows the article with the most sparsely populated neighborhood - i.e. the article with $k$ neighbors whose summed distances is maximum across all other articles.
Although its first neighbor is relatively far away, the features shared between the two are representative for their category, the most prominent being \emph{music}.
The other neighbors appear to only share terms that are somewhat weak in terms of their classification ability as indicated by the differences in category.

As a final comment, it should be observed that both articles would be incorrectly classified using a uniform voting strategy amoung the k neighbors.
However, a correct classification is obtained using a distance-weighted voting strategy.
This might hint towards the benefit of using a distance-weighted voting strategy for sparse-populated neighborhoods, as there might be more noise in a sparse tf-idf vector space than in densely populated ones.

\section{Conclusion}

Based on the analyses of Section \ref{sec:tree} and \ref{sec:knn}, the best performance is achieved by applying $k$-NN classification on tf-idf representations of an article dataset, where the distance between articles is measured either through cosine or Euclidean distance.
Although the sample size of articles is small, the preliminary experiments hint that an assignment of $k \in [ 5, 7 ]$ brings about the best accuracy of approximately 73\%.
Some article categories are better classified when $k \in [5,7]$, whereas others receive better classification for $k \in [3,4]$.
This might suggest that, at classification time, the categories of an article's neighbor should dictate how many neighbors should vote.
For instance, if most of the neighbors are political, then $k$ should be set somewhere in $[5,7]$.
Alternatively, if most of the neighbors are living articles, it may be beneficial to only consider the 4 nearest neighbors.

When considering decision trees, any configuration has shown to perform poorly, never exceeding an accuracy of 60\%, with some categories performing worse than random guessing.
This suggests that, if decisions trees are to be used, some further preprocessing may be necessary to achieve higher accuracies in classification.

\bibliography{bibliography}{}
\bibliographystyle{plain}
\end{document}
