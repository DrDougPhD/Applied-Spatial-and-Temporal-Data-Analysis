\documentclass[11pt]{article}
%Gummi|065|=)
\title{Homework \#2: Discussion on kNN and Decision Trees\\ for Classifying CNN News Articles}
\author{Doug McGeehan\\
		CS 6001: Applied Spatial and \\ Temporal Data Analysis\\
		Spring 2017}

\usepackage[margin=1in]{geometry}
\usepackage{verbatim}
\usepackage{framed}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

\begin{document}

\maketitle

\section{Experimental Setup}

100 articles from the CNN website were transformed into term frequency and tf-idf matrices for this report's analyses.
These articles were selected randomly in such a way as to have an equal portion of articles in each category: since there are seven categories present in the CNN article dataset, each category in the subset of 100 articles consists of 14 to 15 articles.
The results from each experiment were averaged through 5-fold cross validation,
 whereby the dataset was partitioned into five equal-sized subsets, four used for classifier training and one used for classifier testing.
This was repeated five times for all combinations of testing and training partition assignments.
The \texttt{scikit-learn} Python library was used to calculate the precision, recall, and f-scores of each classification task.
Accuracy was calculated using the \texttt{numpy} Python library.

\section{Performance of Decision Trees} \label{sec:tree}

The \texttt{DecisionTreeClassifier} of the \texttt{scikit-learn} Python library was used to perform the experiments on decision tree supervised classification.
This package creates binary-splitting decision trees using either the Gini index or information entropy gain as the criteria for performing node splits.
Once a decision tree is trained, the classes of the articles in the testing dataset are predicted by traversing the tree, with the predicted class compared against its actual class.
Additionally, the path over which the article traversed through the tree is recorded so that the classification difficulty of each class is quantified.

For the illustrations provided in Figures 1 and 2, Minimum Redundancy Maximum Relevance (mRMR) feature selection was applied to the 100 articles so as to reduce their dimensionality to the 100 most relevant features.
This process isolated features such as \emph{president, evidence, Obama}, and \emph{kids} as significant features from which classification may be drawn.
The \texttt{mrmr}\footnote{https://pypi.python.org/pypi/mrmr} Python package was used to perform this feature selection.

% figures/decision_tree/mrmr/path_depths
% figures/decision_tree/mrmr/Term Frequency.prec_n_rec
% figures/decision_tree/mrmr/TF-IDF.prec_n_rec

\begin{figure}[h!] \label{fig:pathlengths}
  \centering
  %\includegraphics[width=\textwidth]{figures/decision_tree/path_depths}
  \includegraphics[width=\textwidth]{figures/decision_tree/mrmr/path_depths}
  \caption{For each article category, the aggregate path lengths over which articles traverse is illustrated.
  Each bar corresponds to these average path lengths, differentiated by how it was constructed: node splitting criterion (entropy or Gini index) and dataset type (term frequency or tf-idf matrices).
  Error lines correspond to the minimum and maximum path lengths.}
\end{figure}


Figure 1 illustrates the average, minimum, and maximum length of the paths through which an article of a given category traversed to be classified.
From this figure, it is observed that articles from the entertainment, travel, and living categories required longer paths to perform classifications on average.
This implies these categories are harder to classify than others.
Beyond those categories, articles from politics and crime were the easiest with their shorter path lengths.
Technology and health articles were moderately difficult to classify.

The figures in Figure 2 illustrate the performance metrics obtained using term frequency and tf-idf representations for training, contrasted against each other based on whether mRMR feature selection was applied to the underlying dataset.
Accuracy, precisions, recalls, and f-scores varied from one criterion-splitting tree to another without a significant overall pattern.

\begin{figure}[h!] \label{fig:perftf_decisiontree}
	\centering
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=\linewidth]{figures/decision_tree/tf_prec_n_rec}
	  \caption{Performance metrics for Decision Trees trained \\
	  on Term Frequency matrices.}
	  \label{fig:sub1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=\linewidth]{figures/decision_tree/tfidf_prec_n_rec}
	  \caption{Performance metrics for Decision Trees trained on Tf-idf matrices with mRMR 100 feature selection.}
	  \label{fig:sub2}
	\end{subfigure}
	\begin{subfigure}{.5\textwidth}
	  \centering
	  %\includegraphics[width=\linewidth]{figures/decision_tree/tf_prec_n_rec}
 	  \includegraphics[width=\linewidth]{figures/decision_tree/mrmr/tf_prec_n_rec}
	  \caption{Performance metrics for Decision Trees trained \\
		on Term Frequency matrices with mRMR 100 \\ feature selection.}
	  \label{fig:sub1}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=\linewidth]{figures/decision_tree/mrmr/tfidf_prec_n_rec}
	  %\includegraphics[width=\linewidth]{figures/decision_tree/tfidf_prec_n_rec}
	  \caption{Performance metrics for Decision Trees trained on Tf-idf matrices with mRMR 100 feature selection.}
	  \label{fig:sub2}
	\end{subfigure}
	\caption{Category-specific performance metrics obtained by Decision Tree classification.}
\end{figure}

Overall, the presence of mRMR feature selection resulted in better performance than when the dataset's sparsity was preserved.
With feature selection, accuracies ranged from 20\% to 63\%, whereas training on an unprocessed dataset received accurate classifications in the range of 16\% to 60\%.
For the remainder of this discussion, the mRMR results are considered.

For decision trees trained on term frequency datasets, accuracies in classification were moderate to low.
Technology, politics, and crime articles received relatively more accurate classifications than other categories by achieving accuracies greater than or equal to 60\%, with crime and technology articles resulting in the highest accuracy of $\approx$65\% on entropy-split trees.
Observing the f-scores of these categories indicates that entropy-split decision trees offered higher precisions and recalls than Gini-split trees.
The categories that performed the poorest were entertainment, travel and living, with accuracies ranging from 20\% to 40\%.
Their f-measures were also noticeably low, not exceeding 30\%.

Decision trees trained on tf-idf representations of the feature-selected dataset, illustrated in Fibure 3d, performed slightly higher than those trained on term frequency representations, in contrast to the performance on an unprocessed dataset in Figure 3b.
Across all article categories, accuracy and f-measures reaching up to 63\% and 72\%, respectively.
Crime achieved the highest precision at 76\% under Entropy-splitting trees, followed closly by politics under Gini-splitting trees at 66\%.
As with the term frequency results, living and entertainment received the poorest performance.
However, between splitting criteria, entropy information gain offered better performance overall than Gini indexes.

Based on these performance measures, it is suggested that entropy-splitting decision trees offers the best performance when considering an mRMR-feature-reduced tf-idf dataset representation.
However, the performance metrics for some categories were still poor under any configuration.
This may be mitigated by performing an mRMR feature selection on each individual category so as to prevent feature domination from the more successful categories.
Due to time constraints, this approach was not investigated.

\section{Discussion on k-NN Classification} \label{sec:knn}

The \texttt{KNeighborsClassifier} of the \texttt{scikit-learn} Python library was used to perform the experiments on $k$-Nearest Neighbors ($k$-NN) supervised classification.
This package measures the distance of an article to be classified against the set of articles with known classes, and predicts the article's class based on its $k$-nearest neighbors.
The distance metric and neighbor voting weights are configurable.
For this analyses, voting was varied between uniformly weighted and distance weighted among all $k$ neighbors.
For calculating the distance between two neighbors, the Jaccard distance (using the \texttt{jaccard} function defined in the \texttt{scipy.spatial.distance} module), cosine distance (\texttt{cosine} from \texttt{scipy.spatial.distance}), and Euclidean distance (defined in \texttt{scikit-learn}) were chosen for \texttt{KNeighborsClassifier}.
The vectors to be trained and tested on were either term frequency vectors representing the articles or tf-idf vectors.

\begin{figure}[h!] \label{fig:perf_knn}
	\centering
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=\linewidth]{figures/knn/accuracies}
	  \caption{Accuracies obtained by varying the $k$ for $k$-NN\\
	   uniform classification between different vector \\ types and different distance metrics.}
	  \label{fig:knn_accuracies}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=\linewidth]{figures/knn/fmeasures}
	  \caption{F-measures obtained by varying the $k$ for $k$-NN\\
	   uniform classification between different vector types and different distance metrics, differentiated by the categories to be classified.}
	  \label{fig:knn_fmeasures}
	\end{subfigure}
	\caption{Accuracies and F-measures obtained from the $k$-NN uniform voting weight series of experiments.}
\end{figure}

Figures 3a and 3b illustrate the performance changes under varying values of $k$ when uniform voting is chosen for neighbor classification.
Figures 4a and 4b illustrate the same, but with distance-weighted voting.
The goal of this experiment was to identify the \emph{optimal} $k$ value for performing supervised classifications (Figure 3a, 4a), as well as observe the f-measures for each particular article category (Figure 3b, 4b).

\begin{figure}[h!] \label{fig:perf_knn_distance}
	\centering
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=\linewidth]{figures/knn/distance_voting/accuracies}
	  \caption{Accuracies obtained by varying the $k$ for $k$-NN\\
	   distance-weighted classification between different \\
	   vector types and different distance metrics.}
	  \label{fig:knn_accuracies}
	\end{subfigure}%
	\begin{subfigure}{.5\textwidth}
	  \centering
	  \includegraphics[width=\linewidth]{figures/knn/distance_voting/fmeasures}
	  \caption{F-measures obtained by varying the $k$ for $k$-NN\\
	   distance-weighted classification between different vector types and different distance metrics, differentiated by the categories to be classified.}
	  \label{fig:knn_fmeasures}
	\end{subfigure}
	\caption{Accuracies and F-measures obtained from the $k$-NN distance-weighted voting series of experiments.}
\end{figure}

In the previous report, it was found that the cosine similarity between term frequency vectors performed the best over the Jaccard similarity and Euclidean distance, with Euclidean performing the poorest.
This is corroborated in the bottom plot of Figure 3a and 4a.
However, the top plots indicate that transforming a term frequency vector into a tf-idf vector brings Euclidean distance almost equal to that of cosine distance.
The Jaccard distance performs the poorest when used between tf-idf vectors, though this is attributed to it not being suitable for real-valued vectors.

\subsection{Optimal $k$}

By varying the value of $k$, the best performance is achieved at $k=5$ to $7$ for tf-idf vectors, with performance dipping slightly at $8 \le k \le 10$ when considering uniform voting weights.
For term frequency vectors, the best performance seems to come at $k=4$ for any configuration.
Intuitively, this suggests that $k<4$ is susceptible to noise and poor correlations between feature occurrences in nearby articles.
For $k>4$, the drop in performance of term frequency vectors suggests that some articles are being incorrectly classified by further-away neighbors of different classes.
%This behavior is more pronounced in Figure 3a when uniform weights were used for classifying a given article, as the accuracies illustrated in Figure 4a appear to remain consistent or growing for $k>4$.

\subsection{Impact of $k$ on F-measures}

Looking at Figure 3b and 4b, the f-measures for each classification is plotted against the changes to $k$.
Overall, travel, entertainment, and living articles receive the worst classifications in both their term frequency and tf-idf representations, implying they have lower precision and recalls.
Politics, crime, and technology articles show consistently high f-measures through each configuration of the experiments.

For $k\ge7$, the precisions and recalls of most categories begin to drop, which impacts the accuracies illustrated in Figures 3a and 4a.
Articles from entertainment, health, living, and travel begin to suffer noticeably.
Classification on crime articles, on the other hand, appears to steadily improve with increasing values of $k$ under distance-weighted voting.
Politics, crime, health, and tech appear to have the highest f-measures at higher values of $k$.

For low $k$ values, the tf-idf representation of technology and politics articles achieves better classifications over others.
Crime starts off with poor performance under distance-weighted voting, but achieves good initial performance under uniform-weighted voting.


\begin{figure}[h!] \label{fig:heatmap_neighborhoods}
  \centering
  \includegraphics[width=\textwidth]{figures/knn/knn_heatmaps}
  \caption{Illustrated here is the feature profiles of two articles with their 5 nearest neighbors.
  Each line in an article's profile represents the occurrence of the article.
  When two articles share a common feature, these line up.
  The bar graphs represents the distance from the article under consideration (the first article) to the its nearest neighbors.
  Finally, the most common feature represents the term that has the highest value according to the corpus' tf-idf values.}
\end{figure}

Figure 5 presents an illustration of the feature occurrences between two articles and the articles within their nearest neighbors.
This figure was created from the experiment on 5-nearest neighbors using Euclidean distance between tf-idf vectors, of which Figure 3 indicates is the optimal configuration for this series of experiments.
For each figure, the first article represents the article to be classified, with the remaining five articles being within each article's $k$-neighborhood.

The top figure shows the article with the most densely populated neighborhood - i.e. the article with $k$ neighbors whose summed distances is minimum across all other articles.
From this figure, we see that it's closest neighbor is within the same class, and shares many common features, the most prominent being the term \emph{drug}.
It is interesting to observe that the remaining neighbors are relatively far away, indicative of their fewer common features and their different categories.
This suggests that the tf-idf vector space is relatively sparse.
However, each article still shares some prominent features, such as \emph{women} and \emph{sexual}, which results in them being closer to the considered article than all others.

The bottom figure shows the article with the most sparsely populated neighborhood - i.e. the article with $k$ neighbors whose summed distances is maximum across all other articles.
Although its first neighbor is relatively far away, the features shared between the two are representative for their category, the most prominent being \emph{music}.
The other neighbors appear to only share terms that are somewhat weak in terms of their classification ability as indicated by the differences in category.

As a final comment, it should be observed that both articles would be incorrectly classified using a uniform voting strategy amoung the k neighbors.
However, a correct classification is obtained using a distance-weighted voting strategy.
This might hint towards the benefit of using a distance-weighted voting strategy for sparse-populated neighborhoods, as there might be more noise in a sparse tf-idf vector space than in densely populated ones.

\section{Conclusion}

Based on the analyses of Section \ref{sec:tree} and \ref{sec:knn}, the best performance with regards to accuracy and f-measure is achieved by applying Entropy-splitting Decision Tree classification on the tf-idf representation of an article dataset, whereby the dimensionality of the dataset was reduced using the Minimum Redundancy Maximum Relevance (mRMR) feature selection process.
The abscense of dimensionality reduction harms the overall performance of classification, with accuracies never exceeding 60\%.
Some categories performed worse than random guessing, falling below 17\%.
For $k$-NN, performance was slightly lower by only a few percentage points.
Although the sample size of articles is small, the preliminary experiments on $k$-NN hint that an assignment of $k \in [ 5, 7 ]$ brings about the best accuracy of approximately 73\% successful classifications.
Some article categories, however, are better classified than others.
For instance, in $k$-NN, articles from living and entertainment would receive better performance when $k \in [3,4]$.
This suggests that, at classification time, the categories of an article's neighbors should dictate how many neighbors should vote.
For instance, if most of the neighbors are political, then $k$ should be set somewhere in $[5,7]$.
Alternatively, if most of the neighbors are living articles, it may be beneficial to only consider the 4 nearest neighbors.

\bibliography{bibliography}{}
\bibliographystyle{plain}
\end{document}
