\documentclass[11pt]{article}
%Gummi|065|=)
\title{Homework \#1: Discussion on the Cosine, \\ Jaccard, and Euclidean Similarities\\ of CNN News Articles}
\author{Doug McGeehan\\
		CS 6001: Applied Spatial and \\ Temporal Data Analysis\\
		Spring 2017}

\usepackage[margin=1in]{geometry}
\usepackage{verbatim}
\usepackage{framed}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}


\begin{document}

\maketitle

\section{Introduction}


\section{News Article Dataset}

For this report, a dataset consisting of over 2000 articles from CNN.com was chosen, with articles spanning January 1st, 2014 to April 4th, 2014\footnote{
https://sites.google.com/site/qianmingjie/home/datasets/cnn-and-fox-news
}.
This dataset was compiled by Qian and Zhai for their research in multi-view unsupervised feature selection on unlabeled text and image data \cite{qian2014unsupervised}.
Articles within the dataset span multiple categories, such as crime, politics, and entertainment.
The number and proportion of articles to each category is provided in Table \ref{tab:categories}.

\begin{table}[h]
	\centering
	\begin{tabular}{ r c c c c  }
		\hline
	Category: &              Politics & Entertainment & Crime & \\ \hline
	Number of Articles: &    409 &      392 &           349 & \\ \hline
	Percentage of Dataset: & 20\% &    19\% &           17\% & \\
	  \hline
	  \hline
	   Health & Travel & Living & Technology \\ \hline
	   286    & 273    & 198    & 148 \\ \hline
	   14\%   & 13.3\% & 10\%   & 7.2\% \\
	\hline
	\end{tabular}
	
	\caption{Number and Proportion of Articles per Category}
	\label{table:categories}
\end{table}


\subsection{Article Contents}

For this report's experiments, the images accompanying each article were discarded and only the textual portion of the articles was analyzed.
Each article is represented as an XML document with tags corresponding to the article's content and metadata.
Of interest is the content within the \texttt{<TEXT></TEXT>} block.
Everything else is ignored.

\pagebreak
\subsection{Example of an Article}

\small 
\begin{quote} \label{samplearticle}
\begin{verbatim}
<DOC>
    <DOCNO>1897</DOCNO>
    <URL>
        http://rss.cnn.com/\~r/rss/cnn\_allpolitics/\~3/
        -AmYNL5GUcc/index.html
    </URL>
    <TITLE>
        Obama to Iran: 'We have the opportunity to start
        down a new path'
    </TITLE>
    <TIME>Thu, 20 Mar 2014 11:10:42 EDT</TIME>
    <ABSTRACT>
        In a message to the Iranian people, an upbeat President
        Barack Obama said Thursday that the long isolated
        Middle East nation can soon improve its economy, its
        world standing and its people's lives if there's a
        breakthrough nuclear deal.
    </ABSTRACT>
    <TEXT>
        (CNN) -- In a message to the Iranian people, an upbeat
        President Barack Obama said Thursday that the long
        isolated Middle East nation can soon improve its
        economy, its world standing and its people's lives if
        there's a breakthrough nuclear deal.
        ...
    </TEXT>
</DOC>
\end{verbatim}
\end{quote}
\normalsize

%\begin{Verbatim}[frame=single]
%\end{Verbatim}

\section{Implementation} \label{sec:software}

Custom-written software was developed for the purpose of performing the experiments of this report. This software was written in the Python 3 programming language and the HTML5 markup language and performs such tasks as automatic extraction of the dataset, corpus transformation into a term-frequency matrix, pairwise article similarity computation, and the presentation of the final sorted results in both an easy to read textual format as well as a web page.
There were many external libraries used to facilitate this functionality.
These libraries are summarized in Table \ref{table:libraries}.

\begin{table}[h] \label{tab:libraries}
	\centering
	\begin{tabular}{ r|c|l }
		\hline
		Environment: & Library & Purpose \\ \hline
		Python: & BeautifulSoup & Parsing the XML of the news articles \\
		&	scikit-learn\cite{scikit-learn} & Corpus to term-frequency matrix transformation \\
		& & and stopword filtering \\
		& 	SciPy\cite{scipy} & Euclidean and Cosine distances \\
		&   numpy & Fast vector operations \\
		&  pickle & Fast reloading of experiment results \\
		& matplotlib & Bar graphs and other visual aids \\ \hline
		Webpage & Bootstrap & Simplified gridwork and design \\
		&  Flask  & Lightweight web server for Python \\
		& Jinja & Web page templating for Python
	\end{tabular}
	
	\caption{Software Libraries and Technologies used in the Software Implementation}
\end{table}

\subsection{Distance Metrics}

The SciPy project \cite{scipy} offers many distance functions operating on a pair of vectors as part of their spatial module\footnote{
https://docs.scipy.org/doc/scipy/reference/spatial.distance.html
}, including one for Euclidean distance %\footnote{
%https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.euclidean.html
%}
and Cosine distance. %\footnote{
%https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html
%}
These functions compute various \emph{dissimilarity} metrics which require mapping to an appropriate similarity metric.
Details on this mapping is provided in Section \ref{sec:mapping}. 

Although there is a distance function named \texttt{jaccard}, it does not compute the Jaccard similarity, but rather the Jaccard-Needham dissimilarity\footnote{
https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jaccard.html
} which cannot be directly mapped to a Jaccard similarity.
Thus, a custom-implemented Jaccard similarity function was developed for this report.
Given two $n$-dimensional vectors $\hat{u} = [ u_1, \cdots, u_n ]$ and $\hat{v} = [ v_1, \cdots, v_n ]$, where $u_i, v_i \ge 0 \, \forall \, 1 \le i \le n$, the Jaccard similarity metric, as is implemented in the custom-implemented Jaccard similarity function, is defined as:

\begin{equation} \label{eq:jaccard}
	s_{\text{jaccard}} = \frac{ \left| \{i \, | \, u_i = v_i \land u_i \ne 0 \} \right| }
	                          { \left| \{i \, | \, u_i = v_i \land u_i = 0 \}^\complement \right| }
\end{equation}
In relation to this report, the Jaccard similarity records the number of times in which one article has the same positive number of occurrences of a particular word as the other article.
This is then scaled by the number of unique words in the union of the two articles. For example, if two articles have the word ``beef'' occur 5 times, then it will be counted in the numerator of Equation \ref{eq:jaccard}.
However, if one article has the word ``dog'' 4 times and the other 3 times, it will not be counted in the numerator, but will be counted in the denominator.
Similarly, the abscence of a word in both documents (i.e. $\exists \, i$ such that $u_i = v_i = 0$) is not counted in either the numerator or the denominator.

For details on the definition of the Euclidean distance and the Cosine distance of two vectors, refer to the SciPi API reference in the footnotes on the previous page.

\subsection{Mapping Distance to Similarity} \label{sec:mapping}

In order to properly use distance metrics as a means to quantify the similarity between two vectors, the distance metrics need to be mapped to some value in the range $[0, 1]$ such that a value near 0 indicates the two vectors have low similarity, and similarity a value near 1 indicates high similarity.
The method in which the distance metrics were normalized is described below.

Let $d_i(u, v)$ denote the distance of vectors $u$ and $v$ as computed by the $i$-th distance method, and let $s_i(u, v)$ denote their corresponding similarity.
For Euclidean distance, the following formula is adopted to map it to a Euclidean similarity:

\begin{equation} \label{eq:euclid}
s_{euclid}(u, v) = \frac{1}{d_{euclid}(u, v) + 1}
\end{equation}
and for cosine distance:
\begin{equation} \label{eq:cosine}
s_{cos}(u, v) = 1 - d_{cos}(u, v)
\end{equation}
The Jaccard similarity was implemented as a similarity metric, and thus there is no need for a mapping.


\section{Experimental Setup and Analysis}

In this section, the results of the similarity analyses is reviewed and discussed.
The software described in Section \ref{sec:software} was executed on a randomly-selected subset of 100 articles from the CNN.com dataset compiled by Qian and Zhai \cite{qian2014unsupervised}.
These articles were transformed into a $100 \times n$ term-frequency matrix $C = [ c_{i,j} ]$, where $n$ is the total number of unique words in the union of all 100 articles, and $c_{i,j}$ represents the number of occurrences of the $j$-th word in the $i$-th article (i.e. $c_{i,j} \ge 0$ for $1 \le i \le 100$ and $1 \le j \le n$).

\subsection{Stopwords: Filtering out Semantically Meaningless Words}

In the English language, there are a subset of words that occur very frequently in dialog or written texts.
A few examples include ``the'', ``a'', ``among'', ``nevertheless'', and ``indeed''.
These are called stopwords and are typically filtered out before or after processing natural language data.
Indeed, the presence of these words skew the results of these experiments because of their frequent reoccurrence.
Figure \ref{fig:stopwords} illustrates the most frequently occurring words in the selected 100-article corpus before (left) and after (right) stopwords are removed.

\begin{figure}[h] \label{fig:stopwords}
  \centering
  \includegraphics[width=\textwidth]{figures/stopwords}
  \caption{The 30 most frequent words in the corpus. The left panel shows a high occurrence of stopwords which offer little semantic meaning. The right panel shows the most frequent words after stopwords defined by the Glasgow Information Retrieval Group were removed.}
\end{figure}

To prevent stopwords from interferring with the similarity experiments, and to accommodate meaningful comparisons between articles, those words occurring more than 300 times were removed from the term-frequency matrix.
Additionally, words that were defined as stopwords by the Glasgow Information
Retrieval Group\footnote{http://ir.dcs.gla.ac.uk/resources/linguistic\_utils/stop\_words} were also removed.
As indicated in the right panel of Figure \ref{fig:stopwords}, there still remains many stopwords in the corpus, such as ``including'', ``he's'', and ``that's''.
Due to time constraints, the list of all present stopswords was not constructed.

\subsection{Results of Euclidean Distance}

The use of the Euclidean Distance, as normalized by the Equation \ref{eq:euclid}, favors pairs of short articles and appears to do a poor job at grouping semantically similar articles together.
Figure \ref{fig:euclid} provides a list of the top 10 article pairs with the highest Euclidean similarity between one another.
These articles have tokenized vectors filled with the most zeros and the lowest article lengths as compared with other articles.
This results in a small Euclidean distance between two vectors, as both vectors are closer to the word space's origin than longer documents, and thus much closer to one another.

\begin{figure}[h] \label{fig:euclid}
  \centering
  \includegraphics[width=\textwidth]{figures/euclidean_most_sim}
  \caption{The top 10 article pairs with the highest Euclidean similarity.
  The right-side axis shows the word that had the most common occurrence in both documents.}
\end{figure}

Of the top 30 closest pairs of documents, every compared article has fewer than 500 words each; 11 have fewer than 200.
It is also observed that there are many repeats of the same article reappearing in higher rankings.
For example, the article titled "Know your Oscar history: Best picture" appears 9 times out of the top 20 closest article pairs.
This flaw in the Euclidean distance becomes even more apparent when one analyzes the common words shared between two documents with the lowest Euclidean distance.
Of the closest 30 pairs of documents, 13 share no words in common.

\subsection{Results of Jaccard Similarity}

Like the Euclidean distance, the Jaccard similarity shows some favoritism towards pairs of short documents, but not to the same extent.
Of the top 30 most similar article pairs, 7 of them both had fewer than 500 words, which is in stark contrast to the results from the Euclidean distance experiment.
This favoritism is due to its use of the total number of unique words as a normalizer of number of shared word observations.
For example, consider one pair of articles with 20 unique words, 5 of which appear the same number of times in both articles.
It's Jaccard similarity would be equal to $\frac{5}{20} = 0.25$.
For a pair of articles with 500 unique words, 5 of which having the same occurrence count, the Jaccard similarity would be equal to 0.01.
This is only a slight favoritism, though.
Of those same 30 articles, 20 of them have 700 or more words each.

\begin{figure}[h] \label{fig:jaccard}
  \centering
  \includegraphics[width=\textwidth]{figures/jaccard_most_sim}
  \caption{The top 10 article pairs with the highest Jaccard similarity.}
\end{figure}

Unlike the Euclidean distance, those articles with the highest similarity appear to share semantic content with one another.
For instance, the most similar pair of articles discusses a scandal involving the actor and filmmaker Woody Allen, and the third most similar pair discusses Russia's involvement in Ukraine during the Crimea annexation.
Within the top 20 pairs of similar articles, 9 of them appear semantically similar with regards to the topic of the articles' discussions.
However, there are false positives in the list mainly attributed to pairs of articles with fewer than 500 words.

\subsection{Results of Cosine Distance}

Figure \ref{fig:cosine} lists the top 10 article pairs that garnered the highest cosine similarity.
The benefit of the Cosine distance is that it doesn't show favoritism towards short documents as do the Jaccard similarity and Euclidean distance metrics.
This is due to its usage of the angle between two article vectors as opposed to their cardinality and length, respectively.
If two articles share many of the same words, they will point towards the same region in the word space and thus have a small angle between them, regardless of how short or long the vector may be.
This trait offers an improvement on its ability to identify semantically similar articles over that of the Jaccard similarity.
With the Jaccard similarity, as applied to term-frequency vectors, a shared word is only counted if the word occurs the same number of times in one document as it does in the other.
Intuitively, this results in many false positives, as semantically similar articles may not have a semantically meaningful word occur the same number of times in both articles.

\begin{figure}[h] \label{fig:cosine}
  \centering
  \includegraphics[width=\textwidth]{figures/cosine_most_sim}
  \caption{The top 10 article pairs with the highest Cosine similarity.}
\end{figure}

The results from the Cosine distance share many of the same document pairs obtaining high similarity scores. For instance, the first and second highest document pairs are on the Woody Allen scandal and Russia's involvement in Ukraine, respectively.
In regards to false positives, the Cosine distance appears to have the lowest of the three.
All of the top 30 ranking article pairs appear to share a similar semantic topic of discussion.

\section{Conclusion}

Between the Jaccard similarity, cosine distance, and Euclidean distance, the results of a pairwise similarity analysis of 100 randomly selected news articles suggest that cosine distance is the best metric for identifying similar articles represented by term-frequency vectors.
However, there are a few instances where the two articles are not in equal tone. For instance, the 32nd ranking article pair primarily focuses on universities, but whereas one discusses notable university buildings in regards to their architectural beauty, the other discusses the death of a student due to a school shooting.
Although this result may be an outlier, it demonstrates the necessity of multiple tools in one's tool belt when performing feature extraction and document similarities.

For future investigations, a good next step would be to use term-frequency, inverse-document-frequency matrix and perform a comparative analysis with the results of using only term-frequency.
The code for performing such an experiment has already been written, but due to time constraints an analysis of the results was omitted from this report.

\begin{figure}[h] \label{fig:articlelengths}
  \centering
  \includegraphics[width=\textwidth]{figures/article_length_dist}
  \caption{The summed lengths of each pair of the articles is plotted to demonstrate the bias towards short articles characteristic of the Euclidean distance. The opposite appears to be true for the Cosine and Jaccard similarity metrics.}
\end{figure}

\bibliography{bibliography}{}
\bibliographystyle{plain}
\end{document}
